{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis using LSTM - pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AWHuXZWtQEXdPK4pQVvpejAWzQ5Lx8Ae",
      "authorship_tag": "ABX9TyOT06KZ/h10F19Z63Y5Olj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshakoul/Examples-of-DL-NLP-using-Pytorch/blob/master/Sentiment_Analysis_using_LSTM_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_YZmYJviUto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "cdb67dba-8117-4050-b519-0bb4c3503522"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSAPLfh8MaW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/train.csv')\n",
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows\n",
        "df_test = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/test.csv')\n",
        "df_test = df_test.sample(frac=1).reset_index(drop=True) #shuffle rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KseJyQ6S1t",
        "colab_type": "text"
      },
      "source": [
        "Pytorch - Text Classification\n",
        "\n",
        "---\n",
        "\n",
        "* Handling OOV words - pytorch supports feature that replaces the rare words in our training data with Unknown token.\n",
        "\n",
        "* Handling Variable Length sequences - As we deal with static networks(models), so we convert the variable length  input sentences into sentences with the same length(threshod also called as sequence length) by adding padding tokens.\n",
        "The issue with padding tokens is that now our network also tries to model the padding like as any other information.\n",
        "This is taken care by **Pytorch's Packed Padding sequence** \n",
        "\n",
        "Packed padding ignores the input timesteps with padding token. These values are never shown to the Recurrent Neural Network which helps us in building a dynamic Recurrent Neural Network.\n",
        "\n",
        "\n",
        "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/Untitled-Diagram1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5YzDmI9WQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#deal with tensors\n",
        "import torch   \n",
        "\n",
        "#handling text data\n",
        "from torchtext import data  \n",
        "\n",
        "#Reproducing same results\n",
        "SEED = 147\n",
        "\n",
        "#Torch\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#Cuda algorithms\n",
        "torch.backends.cudnn.deterministic = True  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1dMS71D9zLb",
        "colab_type": "text"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RogOqHrHMpgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0c7c6a3d-f431-404a-ff50-b93f56440bdc"
      },
      "source": [
        "#Punctuations\n",
        "import string\n",
        "sym = list(string.punctuation)\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IZjVURKp9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download(\"stopwords\")\n",
        "\n",
        "stopwords = set(stopwords.words(\"english\")) | set([\"br\"]) | set(sym)  | set([\"/><br\",'\\'s'])\n",
        "#Tokens to discard during the preprocessing step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDj26tj9yYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy',lower=True,batch_first=True,include_lengths=True,stop_words=stopwords)\n",
        "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
        "fields = [('label', LABEL), ('text',TEXT)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGcE6B9U_GEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = data.TabularDataset(skip_header = True,path='/content/drive/My Drive/iisc/summer/datasets/train.csv',format = 'csv', fields=fields)#defines data stored in csv in terms of column\n",
        "#torchtext.data.dataset.TabularDataset\n",
        "test_data = data.TabularDataset(skip_header = True,path='/content/drive/My Drive/iisc/summer/datasets/test.csv',format = 'csv', fields=fields)#defines data stored in csv in terms of column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpAjci-UAuvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars(train_data.examples[1])) #Defines a single training or test example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPTSGaxrDqRO",
        "colab_type": "text"
      },
      "source": [
        "Creating Vocab and indexing words while taking care of above said problems of OOV words and  variable length sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVtcKzfODXTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3c732fed-470c-4077-d787-05f9a78f8302"
      },
      "source": [
        "#Initializing Glove embeddings\n",
        "\n",
        "TEXT.build_vocab(train_data, min_freq=3, vectors = \"glove.6B.100d\") # Vocab object from one or more datasets.\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 398209/400000 [00:17<00:00, 23983.54it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laAaCbmYE7Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtN-oD-5Qiuo",
        "colab_type": "text"
      },
      "source": [
        "Iterator over batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giZzbeUcPBHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = 64,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True)#,\n",
        "#    device = device)\n",
        "\n",
        "#or use a simply torchtext.data.Iterator(dataset, batch_size, sort_key=None, device=None, batch_size_fn=None, \n",
        "#train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)\n",
        "#https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Iterator \n",
        "\n",
        "#can also use torch.utils.data.DataLoader ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFVBxtXTBwjP",
        "colab_type": "text"
      },
      "source": [
        "**torch.nn.utils.rnn.pack_padded_sequence**(input, lengths, batch_first=False, enforce_sorted=True)\n",
        "\n",
        "* Packs a Tensor containing padded sequences of variable length.\n",
        "* Input can be of size T x B x * where T is the length of the longest sequence \n",
        "* If batch_first is True, B x T x * input is expected.\n",
        "* Parameters:\n",
        "    * input (Tensor) – padded batch of variable length sequences\n",
        "    * lengths (Tensor) – list of sequences lengths of each batch element.\n",
        "    * batch_first\n",
        "    * enforce_sorted (bool, optional) – if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True.\n",
        "* Returns a PackedSequence object    \n",
        "* Internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnlTFZ55Bx0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0abec0c-fe9e-4a0e-c771-d33865424335"
      },
      "source": [
        "#example to understand packed sequence better\n",
        "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
        "c = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])\n",
        "#Data part is just all the tensors concatenated along the time axis. Batch_size is actually the array of batch sizes at each time step\n",
        "#The batch_sizes=[2, 2, 1] represents grouping [1, 3] [2, 4] and [3] respectively(according to time step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([1, 3, 2, 4, 3]), batch_sizes=tensor([2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD2J0T_ZRsH8",
        "colab_type": "text"
      },
      "source": [
        " **Modellling NN**\n",
        " \n",
        " Two functions defined as:\n",
        " * init : arguments passed to the class are initialized by this constructor\n",
        " * forward : defines forward pass of the inputs.\n",
        "\n",
        "**Layers of Model:**\n",
        "* Embedding layer : Get word embeddings for each indexed word, pytorch has nn.embedding which takes input as: \n",
        "    * num_embeddings: No. of unique words in dictionary\n",
        "    * embedding_dim:  No. of dimensions for representing a word (user-defined)\n",
        "\n",
        "* LSTM:\n",
        "Detail explanation of paramaters, input, output of lstm can be found on this stackoverflow [SO link](https://stackoverflow.com/questions/45022734/understanding-a-simple-lstm-pytorch)\n",
        " \n",
        "* Linear Layer\n",
        "* Pack Padding: As explained above, it defines a dynamic recurrent neural network.  It simply ignores the values and returns the hidden state of the non padded element.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OBCWqFjQh0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMNetwork(nn.Module):\n",
        "  #input_size: size of input per time step, in this case size of xt\n",
        "  def __init__(self, output_size, vocab_size, embedding_size, hidden_size, nlayers):\n",
        "    super(LSTMNetwork,self).__init__();\n",
        "    #embedding \n",
        "    #returns a matrix of size vocab_size x embedding_size\n",
        "    self.embeds =  nn.Embedding(vocab_size, embedding_size)  \n",
        "\n",
        "\n",
        "    self.lstm = nn.LSTM(embedding_size, \n",
        "                        hidden_size, \n",
        "                        num_layers=nlayers, \n",
        "                        batch_first=True);\n",
        "   \n",
        "    #fully connected layer\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    #sigmoid layer\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    #text = [batch size,sent_length]\n",
        "    # embedded shape = (batch_size, sentence_length,  embedding_size)\n",
        "    embedded = self.embeds(text) \n",
        "    \n",
        "    #using packed packing sequence\n",
        "    embedding_packed = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)# If batch_first is True, B x T x * input is expected. \n",
        "\n",
        "    #input, (h_0, c_0) where input is of shape (seq_len, batch, input_size) or  also be a packed variable length sequenc\n",
        "    #h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    #c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    #no need to provide as by default they are initialized as zero \n",
        "    packed_output, (final_hidden_state, final_cell_state)  = self.lstm(embedding_packed)\n",
        "    #output, (h_n, c_n): output of shape (seq_len, batch, num_directions * hidden_size) or packedSequence object\n",
        "    #h_n of shape (num_layers * num_directions, batch, hidden_size)  : hidden state when  t = seq_len\n",
        "    #c_n of shape (num_layers * num_directions, batch, hidden_size)  : cell state when  t = seq_len\n",
        "        # final_hidden_state is 1 x batch_size x hidden_size\n",
        "        # [ [h1,h2,h3], [h1,h2,h3]  ] for two reviews, batch = 2, so FINAL hidden state size is 1 x 2 x 3 \n",
        "        # therefore doing final_hidden_state[-1] will give dimesnions as 2x3 and thus can now be easily passed to linear function(fc) as it takes only 2D tensors \n",
        "    \n",
        "    out_fc = self.fc(final_hidden_state[-1]) \n",
        "    #out_fc  will be (batch_size, output_size) : this is similar to FNN\n",
        "    out = self.sigmoid(out_fc);\n",
        "    return out ; "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viCT1nBTez8w",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate the model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en3Cd2HHezO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_size = 1\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_size = 100 #this is TEXT.vocab.vectors.shape\n",
        "hidden_size = 200\n",
        "nlayers = 1\n",
        "model = LSTMNetwork(output_size, vocab_size, embedding_size, hidden_size, nlayers)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6syaanAipbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "80de4d6b-44c9-494a-e247-7f9318db98ae"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMNetwork(\n",
            "  (embeds): Embedding(41753, 100)\n",
            "  (lstm): LSTM(100, 200, batch_first=True)\n",
            "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prglI0YOjngy",
        "colab_type": "text"
      },
      "source": [
        "**Pretrained word embeddings**\n",
        "\n",
        "vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Zz-BoIjm-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d305628-6087-44ee-ae38-25d4461c86ea"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embeds.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape) # each words is rep by 100 size vector"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([41753, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrYLNEHmFML",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate loss and optimizer Class**\n",
        "\n",
        "Here I have used BCE loss as output size of my last linear layer is kept as 1.\n",
        "\n",
        "I could have used CrossEntropyLoss with output size of LL as 2, the CEloss itself calculates softmax and corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSf7XYGamC1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afHd9Z02npWD",
        "colab_type": "text"
      },
      "source": [
        "**TRAINING PHASE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tLBGpr2noRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def trainingNetwork(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0.0\n",
        "  epoch_accuracy = 0.0\n",
        "  \n",
        "  for batch in iterator:\n",
        "    texts, text_lengths = batch.text # we have set inlude_lengths as True while defining data.TEXT\n",
        "    optimizer.zero_grad() \n",
        "    predictions_batch  =  model(texts, text_lengths).squeeze()\n",
        "\n",
        "    #compute the loss\n",
        "    loss = criterion(predictions_batch, batch.label)\n",
        "    acc = binary_accuracy(predictions_batch, batch.label)\n",
        "\n",
        "    #backpropage the loss and compute the gradients\n",
        "    loss.backward()       \n",
        "        \n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip);\n",
        "\n",
        "    #update the weights\n",
        "    optimizer.step()   \n",
        "    #loss and accuracy\n",
        "    epoch_loss += loss.item()  \n",
        "    epoch_accuracy += acc.item()     \n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_accuracy / len(iterator)  "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMDrh7oqGWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_iterator - BucketIterator \n",
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "clip=5\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = trainingNetwork(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}