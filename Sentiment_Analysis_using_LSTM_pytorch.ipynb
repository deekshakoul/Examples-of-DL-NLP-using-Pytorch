{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis using LSTM - pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AWHuXZWtQEXdPK4pQVvpejAWzQ5Lx8Ae",
      "authorship_tag": "ABX9TyNGuhDbsYULzC7UDI2ls8sS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshakoul/Examples-of-DL-NLP-using-Pytorch/blob/master/Sentiment_Analysis_using_LSTM_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_YZmYJviUto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSAPLfh8MaW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/train.csv')\n",
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows\n",
        "df_test = pd.read_csv('/content/drive/My Drive/iisc/summer/datasets/test.csv')\n",
        "df_test = df_test.sample(frac=1).reset_index(drop=True) #shuffle rows"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KseJyQ6S1t",
        "colab_type": "text"
      },
      "source": [
        "Pytorch - Text Classification\n",
        "\n",
        "---\n",
        "\n",
        "* Handling OOV words - pytorch supports feature that replaces the rare words in our training data with Unknown token.\n",
        "\n",
        "* Handling Variable Length sequences - As we deal with static networks(models), so we convert the variable length  input sentences into sentences with the same length(threshod also called as sequence length) by adding padding tokens.\n",
        "The issue with padding tokens is that now our network also tries to model the padding like as any other information.\n",
        "This is taken care by **Pytorch's Packed Padding sequence** \n",
        "\n",
        "Packed padding ignores the input timesteps with padding token. These values are never shown to the Recurrent Neural Network which helps us in building a dynamic Recurrent Neural Network.\n",
        "\n",
        "\n",
        "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/Untitled-Diagram1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5YzDmI9WQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#deal with tensors\n",
        "import torch   \n",
        "\n",
        "#handling text data\n",
        "from torchtext import data  \n",
        "\n",
        "#Reproducing same results\n",
        "SEED = 147\n",
        "\n",
        "#Torch\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#Cuda algorithms\n",
        "torch.backends.cudnn.deterministic = True  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1dMS71D9zLb",
        "colab_type": "text"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RogOqHrHMpgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "85407c7f-c8f2-442f-ffe3-d8077df305ac"
      },
      "source": [
        "#Punctuations\n",
        "import string\n",
        "sym = list(string.punctuation)\n",
        "string.punctuation"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IZjVURKp9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download(\"stopwords\")\n",
        "\n",
        "stopwords = set(stopwords.words(\"english\")) | set([\"br\"]) | set(sym)  | set([\"/><br\",'\\'s'])\n",
        "#Tokens to discard during the preprocessing step"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDj26tj9yYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy',lower=True,batch_first=True,include_lengths=True,stop_words=stopwords)\n",
        "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
        "fields = [('label', LABEL), ('text',TEXT)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGcE6B9U_GEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = data.TabularDataset(skip_header = True,path='/content/drive/My Drive/iisc/summer/datasets/train.csv',format = 'csv', fields=fields)#defines data stored in csv in terms of column\n",
        "#torchtext.data.dataset.TabularDataset\n",
        "test_data = data.TabularDataset(skip_header = True,path='/content/drive/My Drive/iisc/summer/datasets/test.csv',format = 'csv', fields=fields)#defines data stored in csv in terms of column"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpAjci-UAuvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6363a9ba-4201-4033-8ed4-36976dc5f514"
      },
      "source": [
        "print(vars(train_data.examples[1])) #Defines a single training or test example"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'label': '0', 'text': ['wanted', 'see', 'movie', 'article', 'film', 'magazine', \"n't\", 'highly', 'recommended', 'one', 'critic', 'storyline', 'different', 'sure', 'could', 'good', 'movie', 'right', 'hands', 'directing', 'acting', 'awful', 'feeling', 'watching', 'movie', 'made', 'bunch', 'amateurs', 'although', 'movie', 'started', 'promisingly', 'got', 'worse', 'worse', 'think', 'unoriginal', 'movie', 'awkward', 'characters', '..', 'still', 'think', 'worth', 'watching', \"n't\", 'seen', 'films', 'subjecting', 'gay', 'porn', \"n't\", 'keep', 'expectations', 'high', 'though', 'disappointed']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPTSGaxrDqRO",
        "colab_type": "text"
      },
      "source": [
        "Creating Vocab and indexing words while taking care of above said problems of OOV words and  variable length sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVtcKzfODXTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing Glove embeddings\n",
        "TEXT.build_vocab(train_data, min_freq=3, vectors = \"glove.6B.100d\") \n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laAaCbmYE7Iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "0281cd89-74fa-49d8-8d2a-cc6d1d7d7263"
      },
      "source": [
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)   # mapping token strings to numerical identifiers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 41753\n",
            "Size of LABEL vocabulary: 2\n",
            "[('movie', 43063), ('film', 39285), (\"n't\", 32846), ('one', 26135), ('like', 20100), ('good', 14905), ('would', 13458), ('even', 12429), ('time', 12338), ('story', 11725)]\n",
            "defaultdict(<function _default_unk_index at 0x7ff6a81e3b70>, {'<unk>': 0, '<pad>': 1, 'movie': 2, 'film': 3, \"n't\": 4, 'one': 5, 'like': 6, 'good': 7})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtN-oD-5Qiuo",
        "colab_type": "text"
      },
      "source": [
        "Iterator over batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giZzbeUcPBHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "dc9cda09-21c0-4dfc-cfd2-fc453a9ff941"
      },
      "source": [
        "#check whether cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "print(\"cuda? \",torch.cuda.is_available())\n",
        "\n",
        "#A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = 50,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device = device)\n",
        "\n",
        "#or use a simply torchtext.data.Iterator(dataset, batch_size, sort_key=None, device=None, batch_size_fn=None, \n",
        "#train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)\n",
        "#https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Iterator \n",
        "\n",
        "#can also use torch.utils.data.DataLoader ??\n",
        "\n",
        "#explaining about train_iterator\n",
        "''' \n",
        "in batches\n",
        " for every i in batch of 50:\n",
        "    we have a i.text of length 2 \n",
        "        *remember i is basically a train_data.examples[1] that has \"text\" and \"label\"\n",
        "        i.text[0]  -  contains numerical tokens\n",
        "        i.text[1]  -  length of review \n",
        "    we also have i.label that is tensor either of 0 or 1       \n",
        "'''        "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda?  True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\nin batches\\n for every i in batch of 50:\\n    we have a i.text of length 2 \\n        *remember i is basically a train_data.examples[1] that has \"text\" and \"label\"\\n        i.text[0]  -  contains numerical tokens\\n        i.text[1]  -  length of review \\n    we also have i.label that is tensor either of 0 or 1       \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoEe-Hn2TMrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "caa28a36-fe29-4a15-af2d-b85b8052d07c"
      },
      "source": [
        "for i in train_iterator:\n",
        "  print(i.text)\n",
        "  break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[ 253,    3,   99,  ...,   11,    5, 5115],\n",
            "        [ 178,    2,  255,  ...,  619,   16,  589],\n",
            "        [  41,    2,   37,  ...,    4,   54,  259],\n",
            "        ...,\n",
            "        [1844,   43,  113,  ...,    1,    1,    1],\n",
            "        [1195,    7,   37,  ...,    1,    1,    1],\n",
            "        [1367, 4761,  429,  ...,    1,    1,    1]], device='cuda:0'), tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25,\n",
            "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24,\n",
            "        24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23],\n",
            "       device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFVBxtXTBwjP",
        "colab_type": "text"
      },
      "source": [
        "**torch.nn.utils.rnn.pack_padded_sequence**(input, lengths, batch_first=False, enforce_sorted=True)\n",
        "\n",
        "* Packs a Tensor containing padded sequences of variable length.\n",
        "* Input can be of size T x B x * where T is the length of the longest sequence \n",
        "* If batch_first is True, B x T x * input is expected.\n",
        "* Parameters:\n",
        "    * input (Tensor) – padded batch of variable length sequences\n",
        "    * lengths (Tensor) – list of sequences lengths of each batch element.\n",
        "    * batch_first\n",
        "    * enforce_sorted (bool, optional) – if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True.\n",
        "* Returns a PackedSequence object    \n",
        "* Internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnlTFZ55Bx0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#example to understand packed sequence better\n",
        "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
        "c = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])\n",
        "#Data part is just all the tensors concatenated along the time axis. Batch_size is actually the array of batch sizes at each time step\n",
        "#The batch_sizes=[2, 2, 1] represents grouping [1, 3] [2, 4] and [3] respectively(according to time step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD2J0T_ZRsH8",
        "colab_type": "text"
      },
      "source": [
        " **Modellling NN**\n",
        " \n",
        " Two functions defined as:\n",
        " * init : arguments passed to the class are initialized by this constructor\n",
        " * forward : defines forward pass of the inputs.\n",
        "\n",
        "**Layers of Model:**\n",
        "* Embedding layer : Get word embeddings for each indexed word, pytorch has nn.embedding which takes input as: \n",
        "    * num_embeddings: No. of unique words in dictionary\n",
        "    * embedding_dim:  No. of dimensions for representing a word (user-defined)\n",
        "\n",
        "* LSTM:\n",
        "Detail explanation of paramaters, input, output of lstm can be found on this stackoverflow [SO link](https://stackoverflow.com/questions/45022734/understanding-a-simple-lstm-pytorch)\n",
        " \n",
        "* Linear Layer\n",
        "* Pack Padding: As explained above, it defines a dynamic recurrent neural network.  It simply ignores the values and returns the hidden state of the non padded element.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OBCWqFjQh0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMNetwork(nn.Module):\n",
        "  #input_size: size of input per time step, in this case size of xt\n",
        "  def __init__(self, output_size, vocab_size, embedding_size, hidden_size, nlayers,dropout):\n",
        "    super(LSTMNetwork,self).__init__();\n",
        "    #embedding \n",
        "    #returns a matrix of size vocab_size x embedding_size\n",
        "    self.embeds =  nn.Embedding(vocab_size, embedding_size)  \n",
        "\n",
        "\n",
        "    self.lstm = nn.LSTM(embedding_size, \n",
        "                        hidden_size, \n",
        "                        num_layers=nlayers, \n",
        "                        batch_first=True,dropout=dropout);\n",
        "   \n",
        "    #fully connected layer\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    #sigmoid layer\n",
        "    #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    #text = [batch size,sent_length]\n",
        "    # embedded shape = (batch_size, sentence_length,  embedding_size)\n",
        "    embedded = self.embeds(text) \n",
        "\n",
        "    #using packed packing sequence\n",
        "    embedding_packed = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)# If batch_first is True, B x T x * input is expected.\n",
        "\n",
        "    #input, (h_0, c_0) where input is of shape (seq_len, batch, input_size) or  also be a packed variable length sequenc\n",
        "    #h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    #c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    #no need to provide as by default they are initialized as zero \n",
        "    packed_output, (final_hidden_state, final_cell_state)  = self.lstm(embedding_packed)\n",
        "\n",
        "    #output, (h_n, c_n): output of shape (seq_len, batch, num_directions * hidden_size) or packedSequence object\n",
        "    #h_n of shape (num_layers * num_directions, batch, hidden_size)  : hidden state when  t = seq_len\n",
        "    #c_n of shape (num_layers * num_directions, batch, hidden_size)  : cell state when  t = seq_len\n",
        "        # final_hidden_state is 1 x batch_size x hidden_size\n",
        "        # therefore doing final_hidden_state[-1] will give dimesnions as 2x3 and thus can now be easily passed to linear function(fc) as it takes only 2D tensors \n",
        "\n",
        "    out_fc = self.fc(final_hidden_state[-1]) #batch_size x hidden_size\n",
        "    #out_fc  will be (batch_size, output_size) : this is similar to FNN\n",
        "    #out = self.sigmoid(out_fc); # to get op between 0 and 1 as we use BCELoss\n",
        "\n",
        "    return out_fc ; "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viCT1nBTez8w",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate the model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en3Cd2HHezO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_size = 2\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_size = 100 #this is TEXT.vocab.vectors.shape, glove100d\n",
        "hidden_size = 400\n",
        "nlayers = 2\n",
        "dropout=0.4\n",
        "model = LSTMNetwork(output_size, vocab_size, embedding_size, hidden_size, nlayers,dropout)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6syaanAipbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "e940588e-b3f0-4677-86e7-4c04b67e0aeb"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMNetwork(\n",
            "  (embeds): Embedding(41753, 100)\n",
            "  (lstm): LSTM(100, 400, num_layers=2, batch_first=True, dropout=0.4)\n",
            "  (fc): Linear(in_features=400, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prglI0YOjngy",
        "colab_type": "text"
      },
      "source": [
        "**Pretrained word embeddings**\n",
        "\n",
        "vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Zz-BoIjm-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5904bd5b-9125-4dc3-efdc-4cca82d9e47f"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embeds.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape) # each words is rep by 100 size vector"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([41753, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrYLNEHmFML",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate loss and optimizer Class**\n",
        "\n",
        "Here I  have used CrossEntropyLoss with output size of LL as 2, the CEloss itself calculates softmax and corresponding label\n",
        "\n",
        "I could have used BCE loss if output size of my last linear layer is kept as 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSf7XYGamC1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 5e-4"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FilKjhoR82nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afHd9Z02npWD",
        "colab_type": "text"
      },
      "source": [
        "**TRAINING PHASE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tLBGpr2noRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def trainingNetwork(model, iterator, criterion):\n",
        "  epoch_loss = 0.0\n",
        "  epoch_accuracy = 0.0\n",
        "  cou = 0\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    texts, text_lengths = batch.text # we have set inlude_lengths as True while defining data.TEXT\n",
        "    target = batch.label.type(torch.LongTensor)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      texts = texts.cuda()\n",
        "      target = target.cuda()    \n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    predictions_batch  =  model(texts, text_lengths)\n",
        "    \n",
        "    \n",
        "    #compute the loss\n",
        "    loss = criterion(predictions_batch, target)\n",
        "    num_corrects = (torch.max(predictions_batch, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "    acc = 100.0 * num_corrects/len(batch)\n",
        "\n",
        "    #backpropage the loss and compute the gradients\n",
        "    loss.backward()       \n",
        "        \n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), clip);\n",
        "\n",
        "    #update the weights\n",
        "    optimizer.step()   \n",
        "    \n",
        "    #loss and accuracy\n",
        "    epoch_loss += loss.item()  \n",
        "    epoch_accuracy += acc.item()     \n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_accuracy / len(iterator)  "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DQGoPoJ9mEZ",
        "colab_type": "text"
      },
      "source": [
        "Evaluation Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqikVJdF9lfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator):#, criterion):\n",
        "    \n",
        "    #initialize every epoch\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            #retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            target = batch.label.type(torch.LongTensor)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "              text = text.cuda()\n",
        "              target = target.cuda()    \n",
        "\n",
        "            #convert to 1d tensor\n",
        "            predictions = model(text, text_lengths)\n",
        "            \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, target)\n",
        "            #acc = binary_accuracy(predictions, batch.label)\n",
        "            num_corrects = (torch.max(predictions, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            \n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(iterator), total_epoch_acc/len(iterator)\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMDrh7oqGWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "6db227a8-7394-4f3a-bc46-73da5d6644c6"
      },
      "source": [
        "#train_iterator - BucketIterator \n",
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "\n",
        "    train_loss, train_acc= trainingNetwork(model, train_iterator, criterion)\n",
        "    #print(train_loss,\"#\", 'epoch: ',epoch)\n",
        "    #test the model\n",
        "    test_loss, test_acc = evaluate(model, test_iterator)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:3f}, test Acc: {test_acc:.2f}%')    "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 0.680, Train Acc: 57.14%, Test Loss: 0.606280, test Acc: 67.54%\n",
            "Epoch: 02, Train Loss: 0.449, Train Acc: 79.08%, Test Loss: 0.359293, test Acc: 85.17%\n",
            "Epoch: 03, Train Loss: 0.266, Train Acc: 89.28%, Test Loss: 0.291130, test Acc: 87.78%\n",
            "Epoch: 04, Train Loss: 0.171, Train Acc: 93.46%, Test Loss: 0.314287, test Acc: 87.28%\n",
            "Epoch: 05, Train Loss: 0.110, Train Acc: 96.11%, Test Loss: 0.353058, test Acc: 86.96%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
